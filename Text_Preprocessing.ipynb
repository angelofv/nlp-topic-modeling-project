{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPkBfftI4C7z"
      },
      "source": [
        "# Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-07T09:47:54.821988Z",
          "start_time": "2020-12-07T09:47:49.799438Z"
        },
        "id": "kUXE7omA4C7z"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-07T09:47:57.058891Z",
          "start_time": "2020-12-07T09:47:54.841990Z"
        },
        "id": "3ri4U23v4C71"
      },
      "outputs": [],
      "source": [
        "dataset = pd.read_json(\"News_Category_Dataset_v2.json\", lines=True, dtype={\"headline\": str})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-07T09:49:58.874797Z",
          "start_time": "2020-12-07T09:49:58.869804Z"
        },
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "YLCh92YU4C71",
        "outputId": "94b43cf3-f135-497f-894b-4d864e2d5e10"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        category                                           headline  \\\n",
              "0          CRIME  There Were 2 Mass Shootings In Texas Last Week...   \n",
              "1  ENTERTAINMENT  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
              "2  ENTERTAINMENT    Hugh Grant Marries For The First Time At Age 57   \n",
              "3  ENTERTAINMENT  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
              "4  ENTERTAINMENT  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
              "\n",
              "           authors                                               link  \\\n",
              "0  Melissa Jeltsen  https://www.huffingtonpost.com/entry/texas-ama...   \n",
              "1    Andy McDonald  https://www.huffingtonpost.com/entry/will-smit...   \n",
              "2       Ron Dicker  https://www.huffingtonpost.com/entry/hugh-gran...   \n",
              "3       Ron Dicker  https://www.huffingtonpost.com/entry/jim-carre...   \n",
              "4       Ron Dicker  https://www.huffingtonpost.com/entry/julianna-...   \n",
              "\n",
              "                                   short_description       date  \n",
              "0  She left her husband. He killed their children... 2018-05-26  \n",
              "1                           Of course it has a song. 2018-05-26  \n",
              "2  The actor and his longtime girlfriend Anna Ebe... 2018-05-26  \n",
              "3  The actor gives Dems an ass-kicking for not fi... 2018-05-26  \n",
              "4  The \"Dietland\" actress said using the bags is ... 2018-05-26  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-adcd89a9-5c67-4122-a482-4d466ae8a604\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>headline</th>\n",
              "      <th>authors</th>\n",
              "      <th>link</th>\n",
              "      <th>short_description</th>\n",
              "      <th>date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CRIME</td>\n",
              "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
              "      <td>Melissa Jeltsen</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
              "      <td>She left her husband. He killed their children...</td>\n",
              "      <td>2018-05-26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ENTERTAINMENT</td>\n",
              "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
              "      <td>Andy McDonald</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
              "      <td>Of course it has a song.</td>\n",
              "      <td>2018-05-26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ENTERTAINMENT</td>\n",
              "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
              "      <td>Ron Dicker</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n",
              "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
              "      <td>2018-05-26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ENTERTAINMENT</td>\n",
              "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
              "      <td>Ron Dicker</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n",
              "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
              "      <td>2018-05-26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ENTERTAINMENT</td>\n",
              "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
              "      <td>Ron Dicker</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n",
              "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
              "      <td>2018-05-26</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-adcd89a9-5c67-4122-a482-4d466ae8a604')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-adcd89a9-5c67-4122-a482-4d466ae8a604 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-adcd89a9-5c67-4122-a482-4d466ae8a604');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-854558a1-5c4f-4d26-9ae8-35eb84aeccc4\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-854558a1-5c4f-4d26-9ae8-35eb84aeccc4')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-854558a1-5c4f-4d26-9ae8-35eb84aeccc4 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 165
        }
      ],
      "source": [
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vyHr31tBOES",
        "outputId": "deb4d3b1-2fa5-4c8e-95fd-96e23593f50a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 200853 entries, 0 to 200852\n",
            "Data columns (total 6 columns):\n",
            " #   Column             Non-Null Count   Dtype         \n",
            "---  ------             --------------   -----         \n",
            " 0   category           200853 non-null  object        \n",
            " 1   headline           200853 non-null  object        \n",
            " 2   authors            200853 non-null  object        \n",
            " 3   link               200853 non-null  object        \n",
            " 4   short_description  200853 non-null  object        \n",
            " 5   date               200853 non-null  datetime64[ns]\n",
            "dtypes: datetime64[ns](1), object(5)\n",
            "memory usage: 9.2+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeN2Krsw4C75"
      },
      "source": [
        "# Actual pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ib4KBGIO4C76"
      },
      "source": [
        "1. **Ensuring data quality**. We want to make sure that there's no N/A in the data and that everything is in the good format shape.\n",
        "\n",
        "\n",
        "2. **Filtering texts**. We want to get rid of HTML tags or encoding stuff that we don't need in the texts. Before applying anything to them, we need to get them cleaned up.\n",
        "\n",
        "\n",
        "3. **Unifying texts**. In the use case of Topic Modeling, we don't want to make the difference between a word at the beginning of a phrase of in the middle of it here. We should unify all words by lowercasing them and deaccenting them as well.\n",
        "\n",
        "\n",
        "4. **Converting sentences to lists of words**. Some words aren't needed for our analyses, such as *your*, *my*, etc. In order to remove them easily, we have to convert the sentences to lists of words.\n",
        "\n",
        "\n",
        "5. **Remove useless words**. we need to remove useless words from the corpus. We have two approaches: use a hard defined list of stopwords or rely on TF-IDF to identify useless words. The first is the simplest, the second might yield better results.\n",
        "\n",
        "\n",
        "6. **Creating n-grams**. If we look at New York, it is composed of two words. As a result, a word count wouldn't really return a true count for *New York* per se. To fix this, we should represent New York as New_York, which is considered a single word. The n-gram creation consists in identifying words that occur together often and regrouping them. It boosts interpretability for topic modeling in this case.\n",
        "\n",
        "\n",
        "7. **Stemming / Lemmatization**. Shouldn't run, running, runnable be grouped and counted as a single word when we're identifying discussion topics? Yes, they should. Stemming is the process of cutting words to their word root quite brutally while lemmatization will do the same by identifying the kind of word it is working on. We should convert the corpus words into those truncated representations to have a more realistic word count.\n",
        "\n",
        "\n",
        "8. **Part of speech tagging**. POS tagging helps in identifying verbs, nouns, adjectives, and other parts of speech. For topic modeling, it is beneficial to focus on a limited set of parts of speech, such as nouns, proper nouns, verbs, and adjectives. Other parts of speech, like conjunctions and prepositions, typically do not convey significant information about the topics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSP94bTx4C76"
      },
      "source": [
        "## Let's create it!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import os\n",
        "import re\n",
        "import secrets\n",
        "import string\n",
        "\n",
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "from itertools import chain\n",
        "\n",
        "from gensim.models.callbacks import CallbackAny2Vec\n",
        "from gensim.models import Word2Vec, Phrases, KeyedVectors\n",
        "from gensim.models.phrases import Phraser\n",
        "from gensim.utils import simple_preprocess\n",
        "from nltk.corpus import wordnet\n",
        "# from pattern.en import pluralize, singularize\n",
        "from sklearn.feature_extraction import text\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from tqdm import tqdm\n",
        "\n",
        "from spacy.parts_of_speech import IDS as POS_map\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from collections import Counter\n",
        "import matplotlib.patches as mpatches"
      ],
      "metadata": {
        "id": "dQrkvJWgDAVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_D6TmTv4C77"
      },
      "source": [
        "### 1.Ensuring data quality"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to make sure that there's no N/A in the data and that everything is in the good format shape."
      ],
      "metadata": {
        "id": "DiFL5jOfDZ5Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-09T15:18:45.007698Z",
          "start_time": "2020-11-09T15:18:44.995689Z"
        },
        "id": "vA5BuAT74C77"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "def check_data_quality(texts):\n",
        "    \"\"\"Check wheter all the dataset is conform to the expected behaviour.\"\"\"\n",
        "    assert all([isinstance(t, str) for t in texts]), \"Input data contains something different than strings.\"\n",
        "    assert pd.Series(texts).isnull().sum() == 0, \"Input data contains NaN values.\"\n",
        "\n",
        "    return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-09T15:18:46.656382Z",
          "start_time": "2020-11-09T15:18:46.650159Z"
        },
        "id": "vubDg1Lr4C7_"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "def force_format(texts):\n",
        "    return [str(t) for t in texts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-09T15:18:48.842804Z",
          "start_time": "2020-11-09T15:18:48.277827Z"
        },
        "id": "f1w8mmor4C7_"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "texts = force_format(dataset[\"headline\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-09T15:18:50.971008Z",
          "start_time": "2020-11-09T15:18:50.863015Z"
        },
        "id": "Bf-wJyHs4C8B",
        "outputId": "8fca1da6-fb10-49bc-af41-04493dc58832",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is the dataset passing our data quality check?\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "print(f\"Is the dataset passing our data quality check?\\n{check_data_quality(texts)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYwK7JEc4C8B"
      },
      "source": [
        "### 2.Filtering texts"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to get rid of HTML tags or encoding stuff that we don't need in the texts. Before applying anything to them, we need to get them cleaned up."
      ],
      "metadata": {
        "id": "Y9Xjt5AyDsRT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgNHROeQ4C8B"
      },
      "source": [
        "https://regex101.com/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-09T15:18:54.731845Z",
          "start_time": "2020-11-09T15:18:54.711696Z"
        },
        "id": "TMqw3lqX4C8B"
      },
      "outputs": [],
      "source": [
        "def filter_text(texts_in):\n",
        "    \"\"\"Removes incorrect patterns from a list of texts, such as hyperlinks, bullet points and so on\"\"\"\n",
        "\n",
        "    texts_out = re.sub(r'https?:\\/\\/[A-Za-z0-9_.-~\\-]*', ' ', texts_in, flags=re.MULTILINE)\n",
        "    texts_out = re.sub(r'[(){}\\[\\]<>]', ' ', texts_out, flags=re.MULTILINE)\n",
        "    texts_out = re.sub(r'&amp;#.*;', ' ', texts_out, flags=re.MULTILINE)\n",
        "    texts_out = re.sub(r'&gt;', ' ', texts_out, flags=re.MULTILINE)\n",
        "    texts_out = re.sub(r'â€™', \"'\", texts_out, flags=re.MULTILINE)\n",
        "    texts_out = re.sub(r'\\s+', ' ', texts_out, flags=re.MULTILINE)\n",
        "    texts_out = re.sub(r'&#x200B;', ' ', texts_out, flags=re.MULTILINE)\n",
        "    # Mail regex\n",
        "    # This regex is correct but WAY TOO LONG to process. So we skip it with a simpler version\n",
        "    # texts_out = re.sub(r\"(?i)(?:[a-z0-9!#$%&'*+\\/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&'*+\\/=?^_`{|}~-]+)*|\\\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\\\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9]))\\.){3}(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9])|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])\", '', texts_out, flags=re.MULTILINE)\n",
        "    texts_out = re.sub(r'[a-zA-Z0-9-_.]+@[a-zA-Z0-9-_.]+\\.[a-zA-Z0-9-_.]+', '', texts_out, flags=re.MULTILINE)\n",
        "    # Phone regex\n",
        "    # This regex is correct but WAY TOO LONG to process. So we skip it with a simpler version\n",
        "    # texts_out = re.sub(r\".*?(\\(?\\d{3}\\D{0,3}\\d{3}\\D{0,3}\\d{4}).*?\", '', texts_out, flags=re.MULTILINE)\n",
        "    texts_out = re.sub(r\"\\(?\\d{3}\\D{0,3}\\d{3}\\D{0,3}\\d{4}\", '', texts_out, flags=re.MULTILINE)\n",
        "    # Remove names in twitter\n",
        "    texts_out = re.sub(r'@\\S+( |\\n)', '', texts_out, flags=re.MULTILINE)\n",
        "\n",
        "    # Remove starts commonly used on social media\n",
        "    texts_out = re.sub(r'\\*', '', texts_out, flags=re.MULTILINE)\n",
        "    return texts_out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-09T15:19:31.965041Z",
          "start_time": "2020-11-09T15:19:23.674551Z"
        },
        "id": "xTUCDF0D4C8C"
      },
      "outputs": [],
      "source": [
        "texts = [filter_text(t) for t in texts]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSrRn7h3DrDm",
        "outputId": "e40bddf2-db0a-4179-d325-24d218868f60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['There Were 2 Mass Shootings In Texas Last Week, But Only 1 On TV',\n",
              " \"Will Smith Joins Diplo And Nicky Jam For The 2018 World Cup's Official Song\",\n",
              " 'Hugh Grant Marries For The First Time At Age 57',\n",
              " \"Jim Carrey Blasts 'Castrato' Adam Schiff And Democrats In New Artwork\",\n",
              " 'Julianna Margulies Uses Donald Trump Poop Bags To Pick Up After Her Dog',\n",
              " \"Morgan Freeman 'Devastated' That Sexual Harassment Claims Could Undermine Legacy\",\n",
              " \"Donald Trump Is Lovin' New McDonald's Jingle In 'Tonight Show' Bit\",\n",
              " 'What To Watch On Amazon Prime That’s New This Week',\n",
              " \"Mike Myers Reveals He'd 'Like To' Do A Fourth Austin Powers Film\",\n",
              " 'What To Watch On Hulu That’s New This Week']"
            ]
          },
          "metadata": {},
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBko_Rxg4C8C"
      },
      "source": [
        "### 3.Unifying texts & 4.converting sentences to list of words"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the use case of Topic Modeling, we don't want to make the difference between a word at the beginning of a phrase of in the middle of it here. We should unify all words by lowercasing them and deaccenting them as well.\n",
        "\n",
        "Some words aren't needed for our analyses, such as your, my, etc. In order to remove them easily, we have to convert the sentences to lists of words."
      ],
      "metadata": {
        "id": "E1a126d0E2s1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sent_to_words(sentences):\n",
        "    \"\"\"Converts sentences to words.\n",
        "\n",
        "    Convert sentences in lists of words while removing the accents and the punctuation.\n",
        "\n",
        "    @param:\n",
        "        sentences: a list of strings, the sentences we want to convert\n",
        "    @return\n",
        "        A list of words' lists.\n",
        "    \"\"\"\n",
        "    for sentence in tqdm(sentences): # tqdm show a bar progress over iterable\n",
        "        yield (simple_preprocess(str(sentence), deacc=True,)) # lowercase and remove accent and number"
      ],
      "metadata": {
        "id": "_PA66fVAx6-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = list(sent_to_words(texts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sR8-3723x8Um",
        "outputId": "af92ff21-f253-40ba-9617-3269c8745a63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200853/200853 [00:06<00:00, 29752.17it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5_MbcLZD2Jq",
        "outputId": "ab68fbe4-205d-4000-add4-723ee86e362a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['there',\n",
              "  'were',\n",
              "  'mass',\n",
              "  'shootings',\n",
              "  'in',\n",
              "  'texas',\n",
              "  'last',\n",
              "  'week',\n",
              "  'but',\n",
              "  'only',\n",
              "  'on',\n",
              "  'tv'],\n",
              " ['will',\n",
              "  'smith',\n",
              "  'joins',\n",
              "  'diplo',\n",
              "  'and',\n",
              "  'nicky',\n",
              "  'jam',\n",
              "  'for',\n",
              "  'the',\n",
              "  'world',\n",
              "  'cup',\n",
              "  'official',\n",
              "  'song'],\n",
              " ['hugh', 'grant', 'marries', 'for', 'the', 'first', 'time', 'at', 'age']]"
            ]
          },
          "metadata": {},
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABpm74mr4C8C"
      },
      "source": [
        "### 5.Removing useless words"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to remove unnecessary words from the corpus. We will use a predefined list of stop words from *sklearn.feature_extraction* and add a custom list for simplicity."
      ],
      "metadata": {
        "id": "UBfBCxczIOhp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-09T15:26:10.796788Z",
          "start_time": "2020-11-09T15:26:10.786126Z"
        },
        "id": "VwYAFN2f4C8C"
      },
      "outputs": [],
      "source": [
        "def get_stopwords(additional_stopwords=[]):\n",
        "    \"\"\"Return a list of english stopwords, that can be augmented by using a stopwords file or a list of stopwords\n",
        "\n",
        "    Args:\n",
        "        filepath (str, optional): path to a text file where each line is a stopword\n",
        "        additional_stopwords (list of str, optional): list of string representing stopwords\n",
        "    Returns:\n",
        "        List of strings representing stopwords\n",
        "    \"\"\"\n",
        "    # Loading standard english stop words\n",
        "    with open('stopwords.txt', 'r') as f:\n",
        "        stop_w = f.readlines() # return a list stop_w where each line of the file correspond to an element of the list\n",
        "    stopwords = [s.rstrip() for s in stop_w] # removing trailing new line \"\\n\" character\n",
        "\n",
        "    # Adding stop words from sklearn\n",
        "    stopwords = list(text.ENGLISH_STOP_WORDS.union(stopwords))\n",
        "\n",
        "    # Adding words from a list if specified\n",
        "    if additional_stopwords:\n",
        "        stopwords += additional_stopwords\n",
        "\n",
        "    # Removing duplicates\n",
        "    stopwords = list(set(stopwords))\n",
        "\n",
        "    # Removing some \\n that were included in the native stopwords of sklearn ... WHY?\n",
        "    stopwords = [s.replace(\"\\n\", \"\") for s in stopwords]\n",
        "\n",
        "    stopwords = sorted(stopwords, key=str.lower)\n",
        "\n",
        "    return stopwords\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = get_stopwords(additional_stopwords=[\"trump\",\"(PHOTOS)\",\"donald\"])"
      ],
      "metadata": {
        "id": "jbNMvZ--uh0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [[word for word in txt if word not in stopwords] for txt in tqdm(texts)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qfq1Q0VLM9jE",
        "outputId": "b5bf3e02-2c10-44e0-e247-0072099dc413"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200853/200853 [00:19<00:00, 10308.94it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JB9YaeUbEP2T",
        "outputId": "473c133f-eb8a-4a14-d9c4-cead85880bed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['mass', 'shootings', 'texas', 'week', 'tv'],\n",
              " ['smith',\n",
              "  'joins',\n",
              "  'diplo',\n",
              "  'nicky',\n",
              "  'jam',\n",
              "  'world',\n",
              "  'cup',\n",
              "  'official',\n",
              "  'song'],\n",
              " ['hugh', 'grant', 'marries', 'time', 'age']]"
            ]
          },
          "metadata": {},
          "execution_count": 191
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xNLG-J34C8D"
      },
      "source": [
        "### 6.Creating n-grams"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we will focus on creating bigrams"
      ],
      "metadata": {
        "id": "_zasmkqCRTuL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-09T15:29:37.456130Z",
          "start_time": "2020-11-09T15:29:37.443129Z"
        },
        "id": "9-DgGm-r4C8D"
      },
      "outputs": [],
      "source": [
        "def create_bigrams(texts, bigram_count=15, threshold=10, convert_sent_to_words=False, as_str=True):\n",
        "    \"\"\"Identify bigrams in texts and return the texts with bigrams integrated\"\"\"\n",
        "    if convert_sent_to_words:\n",
        "        texts = list(sent_to_words(texts))\n",
        "\n",
        "    bigram_model = Phraser(Phrases(texts, min_count=bigram_count, threshold=threshold))\n",
        "\n",
        "    if as_str:\n",
        "        return [\" \".join(bigram_model[t]) for t in texts]\n",
        "\n",
        "    else:\n",
        "        return [bigram_model[t] for t in texts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-09T15:35:40.170188Z",
          "start_time": "2020-11-09T15:35:15.893971Z"
        },
        "id": "2moBdLP64C8E"
      },
      "outputs": [],
      "source": [
        "texts = create_bigrams(texts)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjZ11YQlB5s_",
        "outputId": "88a2e989-c771-403f-ae0a-c33c229b46c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['mass_shootings texas week tv',\n",
              " 'smith joins diplo nicky jam world_cup official song',\n",
              " 'hugh grant marries time age']"
            ]
          },
          "metadata": {},
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0Vx-q9l4C8E"
      },
      "source": [
        "### 7.Stemming/Lemmatization & 8.Part-of-Speech filtering"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we will use en_core_web_lg, which is a spaCy model, to perform lemmatization and part-of-speech (POS) tagging. We will filter the text to retain only the following parts of speech: NOUN, ADJ, VERB, ADV, and PROPN."
      ],
      "metadata": {
        "id": "KfjCaIPeLRdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "id": "iztXiLnvKI07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81186e1c-cbf6-4e2e-da94-94545175d345"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-18 18:14:31.571121: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-lg==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.6.0/en_core_web_lg-3.6.0-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-lg==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-07T07:31:11.576915Z",
          "start_time": "2020-12-07T07:31:11.572949Z"
        },
        "id": "UznQ2j0q4C8E"
      },
      "outputs": [],
      "source": [
        "def lemmatize_texts(texts,\n",
        "                    allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'],\n",
        "                    forbidden_postags=False,\n",
        "                    as_sentence=False,\n",
        "                    get_postags=False,\n",
        "                    spacy_model=False):\n",
        "    \"\"\"Lemmatize a list of texts.\n",
        "\n",
        "            Please refer to https://spacy.io/api/annotation for details on the allowed\n",
        "        POS tags.\n",
        "        @params:\n",
        "            - texts: a list of texts, where each texts is a string\n",
        "            - allowed_postags: a list of part of speech tags, in the spacy fashion\n",
        "            - as_sentence: a boolean indicating whether the output should be a list of sentences instead of a list of word lists\n",
        "        @return:\n",
        "            - A list of texts where each entry is a list of words list or a list of sentences\n",
        "        \"\"\"\n",
        "    texts_out = []\n",
        "\n",
        "    if allowed_postags and forbidden_postags:\n",
        "        raise ValueError(\"Can't specify both allowed and forbidden postags\")\n",
        "\n",
        "    if forbidden_postags:\n",
        "        allowed_postags = list(set(POS_map.keys()).difference(set(forbidden_postags))) # return a list of POS tags that are in POS_map but not in forbidden_postags\n",
        "\n",
        "    if not spacy_model:\n",
        "        print(\"Loading spacy model\")\n",
        "        spacy_model = spacy.load('en_core_web_lg') #en_core_web_trf\n",
        "\n",
        "    print(\"Beginning lemmatization process\")\n",
        "    total_steps = len(texts)\n",
        "\n",
        "    docs = spacy_model.pipe(texts)\n",
        "\n",
        "    for doc in tqdm(docs, total=total_steps):\n",
        "        if get_postags:\n",
        "            texts_out.append([\"_\".join([token.lemma_, token.pos_]) for token in doc if token.pos_ in allowed_postags])\n",
        "        else:\n",
        "            texts_out.append(\n",
        "                [token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "\n",
        "    if as_sentence:\n",
        "        texts_out = [\" \".join(text) for text in texts_out]\n",
        "\n",
        "    return texts_out\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l_texts = lemmatize_texts(texts[:1000],\n",
        "                allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV','PROPN'],\n",
        "                get_postags=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFrvlGDJPgJT",
        "outputId": "9e979dc4-a8d8-4172-82ef-be8dfd5840f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading spacy model\n",
            "Beginning lemmatization process\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:02<00:00, 482.22it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove empty headlines\n",
        "l_texts = [headline for headline in l_texts if headline]"
      ],
      "metadata": {
        "id": "C4b6CxQf3t0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l_texts[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1V8T8qnPhqG",
        "outputId": "d986bf9f-7fa4-4796-e97f-22176a5eafe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['mass_shootings', 'texas', 'week', 'tv'],\n",
              " ['smith', 'join', 'diplo', 'nicky', 'jam', 'world_cup', 'official', 'song'],\n",
              " ['hugh', 'grant', 'marry', 'time', 'age']]"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exporting Processed Texts to JSON File"
      ],
      "metadata": {
        "id": "bJ67oSX11njG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "# File path where you want to save the file\n",
        "file_path = 'l_texts_en_core_web_lg.json'\n",
        "\n",
        "# Writing to the file\n",
        "with open(file_path, 'w') as file:\n",
        "    # Serializing the list of lists as JSON\n",
        "    json.dump(l_texts, file)"
      ],
      "metadata": {
        "id": "c2EdveT00X_H"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}